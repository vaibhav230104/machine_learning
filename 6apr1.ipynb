{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481cb9ce-9a17-4b08-a84b-ae6f65227d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is the mathematical formula for a linear SVM?\n",
    "\n",
    "# The mathematical formula for a linear SVM aims to find the hyperplane represented as:\n",
    "# w * x + b = 0\n",
    "# where 'w' is the weight vector, 'x' is the input feature vector, and 'b' is the bias or intercept term.\n",
    "\n",
    "# Q2. What is the objective function of a linear SVM?\n",
    "\n",
    "# The objective function of a linear SVM is to maximize the margin while minimizing the norm of the weight vector. It can be represented as minimizing 1/2 * ||w||^2 subject to yi(w * xi + b) >= 1 for all training examples.\n",
    "\n",
    "# Q3. What is the kernel trick in SVM?\n",
    "\n",
    "# The kernel trick in SVM allows handling nonlinear decision boundaries by implicitly mapping input features to a higher-dimensional space. It avoids explicitly computing transformed features but computes dot products efficiently using kernel functions like linear, polynomial, RBF, or sigmoid.\n",
    "\n",
    "# Q4. What is the role of support vectors in SVM? Explain with an example.\n",
    "\n",
    "# Support vectors are the data points closest to the decision boundary (margin) and play a crucial role in defining the hyperplane. They determine the margin and the orientation of the hyperplane. For example, in a binary classification scenario, support vectors are the instances that lie on the boundary or are misclassified.\n",
    "\n",
    "# Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin, and Hard margin in SVM?\n",
    "\n",
    "# Visualization of Hyperplane, Marginal Plane, Soft Margin, and Hard Margin can be challenging without using specific visualization tools in higher dimensions. Hyperplane is the decision boundary, the Marginal Plane is the region between support vectors, Soft Margin allows for some misclassifications, and Hard Margin doesn't tolerate any misclassifications.\n",
    "\n",
    "# Q6. SVM Implementation through Iris dataset.\n",
    "\n",
    "# Steps for implementing SVM with Iris dataset:\n",
    "# 1. Load the Iris dataset using scikit-learn.\n",
    "# 2. Split it into training and testing sets.\n",
    "# 3. Train a linear SVM classifier using scikit-learn.\n",
    "# 4. Predict labels for the test set and compute accuracy.\n",
    "# 5. Plot decision boundaries using two features.\n",
    "# 6. Experiment with different values of the regularization parameter (C) and observe their impact on model performance.\n",
    "\n",
    "# Bonus Task: Implementing a Linear SVM Classifier from Scratch\n",
    "\n",
    "# - Load the Iris dataset from scikit-learn.\n",
    "# - Split it into a training and testing set.\n",
    "# - Implement a linear SVM classifier from scratch in Python.\n",
    "# - Train the model on the training set and predict labels for the testing set.\n",
    "# - Compute the accuracy of the model on the testing set.\n",
    "# - Plot decision boundaries using two features.\n",
    "# - Experiment with different regularization parameter (C) values to observe their impact on model performance.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
