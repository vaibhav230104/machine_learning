{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c92e17f3-3e1c-449b-b95b-042934891aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is Ridge Regression, and how does it differ from ordinary least squares regression?\n",
    "\n",
    "# Ridge Regression is a regularization technique used to mitigate multicollinearity and overfitting in regression models by adding a penalty term (L2 norm) to the ordinary least squares (OLS) objective function. Unlike OLS, Ridge Regression introduces a regularization term that prevents coefficients from becoming too large.\n",
    "\n",
    "# Q2. What are the assumptions of Ridge Regression?\n",
    "\n",
    "# The assumptions of Ridge Regression are similar to ordinary least squares regression: linearity, independence of errors, homoscedasticity (constant variance of residuals), and normally distributed residuals. Additionally, Ridge Regression assumes that multicollinearity might be present among the independent variables.\n",
    "\n",
    "# Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression?\n",
    "\n",
    "# The value of the tuning parameter (lambda or Î±) in Ridge Regression can be selected using techniques like cross-validation. By performing cross-validation on different values of lambda, one can choose the value that minimizes the prediction error or uses other criteria like the one-standard-error rule or grid search.\n",
    "\n",
    "# Q4. Can Ridge Regression be used for feature selection? If yes, how?\n",
    "\n",
    "# Ridge Regression tends to shrink coefficients toward zero but doesn't make them exactly zero. Therefore, it doesn't perform explicit feature selection. However, it reduces the impact of less relevant features by shrinking their coefficients, making them less influential in the model.\n",
    "\n",
    "# Q5. How does the Ridge Regression model perform in the presence of multicollinearity?\n",
    "\n",
    "# Ridge Regression is particularly useful in handling multicollinearity by stabilizing the coefficients. It reduces the variance of the coefficients by adding a penalty term, allowing the model to perform better than OLS when multicollinearity is present.\n",
    "\n",
    "# Q6. Can Ridge Regression handle both categorical and continuous independent variables?\n",
    "\n",
    "# Yes, Ridge Regression can handle both categorical and continuous independent variables. Categorical variables need to be appropriately encoded (e.g., one-hot encoding) to be used in the model.\n",
    "\n",
    "# Q7. How do you interpret the coefficients of Ridge Regression?\n",
    "\n",
    "# The coefficients in Ridge Regression represent the relationship between the independent variables and the dependent variable, considering the penalty applied to shrink the coefficients. Larger coefficients indicate stronger relationships, but interpretation becomes complex due to the penalty term.\n",
    "\n",
    "# Q8. Can Ridge Regression be used for time-series data analysis? If yes, how?\n",
    "\n",
    "# Yes, Ridge Regression can be applied to time-series data analysis to handle multicollinearity and overfitting. It can include lagged variables or other time-based features as independent variables, treating them similarly to other predictors in the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49783a04-0015-4187-9a4f-e24ae4e18448",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
